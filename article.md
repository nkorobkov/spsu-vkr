# Метод межязыковой адаптации диалоговых систем
*Коробков Никита*

\documentclass[14pt]{matmex-diploma-custom}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{misccorr}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tocloft}
\usepackage{wrapfig}
\graphicspath{ {images/} }

\newcommand{\nextline}{\tabularnewline\hline}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}

\filltitle{ru}{
	chair              = {Кафедра Технологии программирования},
	title              = {Метод межъязыковой адаптации диалоговых систем},
	type               = {bachelor},
	position           = {студента},
	group              = {Направление 01.03.02 \\ Прикладная математика и информатика},
	author             = {Коробков Никита Александрович},
	supervisorPosition = {старший преподаватель},
	supervisor         = {Мишенин~А.Н.},
	chairHeadPosition  = {д.\,ф.-м.\,н., профессор},
	chairHead          = {},
	faculty            = {Факультет Прикладной математики - Процессов управления}
}

\maketitleru{}
\tableofcontents

\newgeometry{top=40mm,bottom=20mm,left=25mm,right=25mm,nohead,includeheadfoot}


\section*{Введение}
В последние годы все больше внимания уделяется решению различных задач с применением машинного обучения и нейронных сетей. Одним из наиболее интересных направлений в области применения современных нейросетевых методов являются диалоговые системы.\par
Диалоговая система это набор программ и алгоритмов позволяющих человеку вести диалог с программой в манере свойственной человеческой. Иногда диалоговые системы называют разговорным искусственным интеллектом или "чат-ботом". \par
Задача построения подобных программ является актуальной для промышленной области потому, что решение задач пользователей путем диалога с агентом поддержки всегда было и будет оставаться наиболее простым и эффективным методом. Так как вербальное общение наиболее естественный для человека способ коммуникации. Построение хорошей и надежной диалоговой системы заменяющей работников колл центра позволило бы существенно сократить затраты ресурсов.  \par
Диалоговые системы можно разделить на системы общего назначения и задачеориентированные. Чем более узкую задачу призвана решать система, тем проще она может быть устроена. В самом тривиальном случае система может просто возвращать заранее известный ответ. Например текущее время.  \par
Мы хотели бы построить систему решающую более общую задачу. Для этого в нужно сначала выяснить детально чего именно хочет пользователь (задать вопрос и получить ответ). В самой тривиальной реализации этот сценарий может выглядеть как простой выбор задачи из списка. С применением такого подхода в промышленности можно столкнуться уже сегодня. Позвонив в банк и авиакомпанию можно часто услышать робота, который предложит нажать разные кнопки в зависимости от цели звонка ("Для проверки баланса нажмите 1, для уточнения статуса заявки нажмите 2" и.т.п.). Этот подход хорош тем, что не требует от системы никакой интеллектуальности и работает очень надежно. Однако занимает много пользовательского времени и зачастую нервирует. В идеальном случае мы хотели бы получив запрос в виде предложения на натуральном языке, например "Какая погода сейчас на улице?" сразу распознать намерение пользователя. \par
Это одна из подзадач в построении диалоговых систем, на которой хотелось бы сконцентрировать наше внимание в этой работе. 
Данная задача является довольно актуальной и стоит уже давно. Так что для ее решения было предложено множество методик. Подробнее они будут рассмотрены в разделе "Обзор литературы". Большинство методов ориентировано на работу с английским языком. В основном потому, что для английского собрано наиболее большое количество данных. Кроме того английский просто считается языком по умолчанию в научной среде.  \par
В то время как для английского языка достигнуты внушительные результаты, ситуация с другими языками обстоит несколько иначе. Представленные модели в большинстве своем обучены на колоссальном объеме размеченных данных. Таких данных не существует для более редких языков. По этому простое перенесение достигнутых результатов путем обучения идентичной модели на другом языке не представляется возможным.  
Тем не менее, почти для всех мировых языков существуют полные словари. Пользуясь общими знаниями о связи двух языков (словари, параллельные тексты) можно обобщить знания одной модели на другой язык не используя размеченных данных для второго языка совсем (либо используя совсем немного). Данный подход в литературе носит название Transfer Learning.
Целью данной работы будет построение модели для извлечения намерения из предложения на шведском языке при помощи переноса знаний накопленных обученной на английском языке модели. Выработанную методику можно будет использовать для построения моделей приблизительно такой же точности для любого другого языка для которого существует словарь перевода слов на английский. 




\newpage
\section*{Постановка задачи}
Пусть существует множество команд на английском языке \(E\) и конечное множество намерений \(K\)

\[ 
    K = \{k_1,k_2,\dots,k_n\}
\]

Каждой команде из \(E\) однозначно соответствует элемент множества \(K\). Соответствие обозначим \(J_e\)
\[ 
    J_e : E \to K
\]
Тренировочные данные состоят из множеств \(E, K\) и соответствия \(J_e\).  При этом существует также функция-переводчик \(T\), которая каждой команде на английском языке ставит в соответствие команду на шведском языке. Множество команд на шведском языке обозначим \(S\)
\[ 
    T : E \to S
\]
Целью данной работы будет получение функции \(J_s : S \to K \) сопоставляющей любой команде на шведском языке намерение. При этом должны выполняться два условия. 

1.  Намерение должно совпадать с намерением перевода шведской команды на английский язык. 

	\begin{equation} \label{eq:reverse_is}
	\forall s \in S \quad J_s(s) = J_e(T^{-1}(s)) 
	\end{equation}
    \begin{equation} \label{eq:straight_is}
	\forall e \in E \quad J_s(T(e)) = J_e(e)
	\end{equation}

2. Функция \(J_s\) не должна зависеть от функции \(T\)

Второе условие является определяющим для данной задачи. Если бы мы могли использовать функцию \(T\) в \(J_s\) то можно было бы просто определить \(J_s\) как в \eqref{eq:reverse_is} и остановиться на этом.  
Но это невозможно, так как перевод (вычисление \(T^{-1}\)) это слишком дорогостоящая операция. Мы бы хотели получить функцию которая была бы достаточно легко вычислимой для использования в мобильных приложениях. Поэтому вместо прямого перевода команды со шведского языка на английский и последующего применения существующих алгоритмов мы постараемся выделить какие-то ключевые атрибуты команды на шведском языке и использовать их для распознавания намерения. \par
В силу требования 2 мы едва ли сможем удовлетворить требование 1 полностью. Вместо этого попытаемся построить систему, которая на тренировочных данных сможет максимально часто предсказывать намерения пользователя правильно. В качестве метрики качества предсказаний будем использовать долю команд в тестовой выборке, по которой система приняла правильное решение. \par

\newpage
\section*{Обзор литературы}
Задача построения диалоговых систем лежит в области автоматического анализа текстовых данных. Одним из главных вопросов применения нейронных сетей в данной области является эффективное представление слов в памяти компьютера. При анализе текстов мы бы хотели заменять слова на вектора как-то отражающие семантический смысл слова.  
Наиболее распространенный метод получения векторов слов описан в работах \cite{w2v}
В данном подходе обучающий текстовый корпус просматривается окном ширины 2h + 1 слов, и для каждого окна однослойная нейронная сеть предсказывает центральное слово окна w(t) по окружающим w(t + i), i ∈ [−h, h] или
наоборот. Эти архитектуры называются Continuous Bag-of-words и Skipgram соответственно. Минимизируя ошибку предсказания, нейронная сеть строит проекцию слов в векторное пространство заранее определенной размерности. При достижении заданной точности предсказания или определенного
числа эпох, алгоритм генерирует словарь с векторными представлениями для слов из обучающего корпуса.

\begin{figure}[h]
\caption{Архитектуры нейронных сетей, представленные в \cite{w2v} для окна ширины
h = 5}
\centering
\includegraphics{word2vec.png}
\end{figure}

Данный подход позволяет получить вектора обладающие свойством семантической близости. Мы можем надеяться что слова обладающие схожим смыслом будут находится рядом в построенном векторном пространстве.  
После того как методы описанные в \cite{w2v} показали свою эффективность в ряде задач обработки текстов \cite{LM}. Было разработано и предложено несколько похожих методов построения векторов слов \cite{elmo}, \cite{glove}

В данный момент большинство методик обработки естественного языка так или иначе использует вектора слов. 

В нашей работе мы использовали дополненную реализацию оригинального word to vec "fasttext" \cite{fasttext}. В отличие от оригинальной архитектуры этот подход помимо слов контекста использует части слов для обучения, что позволяет предсказывать вектора слов для слов отсутствующих в тренировочной коллекции. 

Для работы с последовательностями слов (предложениями) часто применяются рекуррентные нейронные сети описанные в статьях \cite{rnn} \cite{rnn2}. Во Избежание проблемы затухающих градиентов при обучении, используют LSTM архитектуру \cite{lstm}.

В статье Attention is all you need \cite{attention} группа исследователей из Google описывает принципиально новый подход к обработке последовательной текстовой информации и в частности к переводам. Вместо классической архитектуры рекуррентных нейронных сетей с использованием LSTM или GRU модулей автор использует так называемый механизм внимания, Который позволяет более качественно представлять информацию содержащуюся в предложениях на этапе кодирования. Такую сеть так же называют "Трансформер" из за гибкости внутренней структуры, позволяющей получать разную информацию о кодируемом предложении в зависимости от запроса. Многие современные автоматические переводчики пользуются этой технологией. В нашей работе мы пользовались готовой системой автоматического перевода от  Яндекс. Примерное описание механизмов работы их переводчика доступно в статье \cite{ytranslate}

Идея трансформер сетей развивается в статье \cite{bert}. Авторы предлагают тренировать многослойную модель из трансформер модулей на задаче определения связности предложений и предсказания пропущенного слова. Полученная модель показывает исключительные результаты после дообучения на ряде конкретных задач. 

Задача предсказания намерения из фиксированного множества может быть сформулирована как задача классификации предложений. Интересный подход к этой задаче с использованием сверточных сетей предложен в статье \cite{deepPavlovCNN}.

\newpage
\section{Обработка данных}

В нашей работе в качестве тренировочных и тестовых данных мы использовали датасет собранный компанией SNIPS \cite{snipsData}. 
Данные включают в себя более 13000 запросов пользователей на английском языке. Каждый запрос относится к одной из семи категорий по намерению пользователя.  

Представленные намерения:
\begin{itemize}
  \item Get weather (Посмотреть погоду - 2000 записей)
  \item Play music (Включить музыку - 2000 записей)
  \item Book restaurant (Забронировать ресторан - 1973 записей)
  \item Search creative work (Поиск произведений - 1954 записей)
  \item Add to playlist (Добавить в плейлист - 1942 записей) 
  \item Rate book (Поставить оценку книге - 1956 записей)
  \item Search movie schedule (Расписание сеансов кино - 1959 записей)
\end{itemize}

Данные предоставлены компанией SNIPS по лицензии Creative Commons Zero v1.0 Universal и доступны для скачивания по ссылке \url{https://github.com/snipsco/nlu-benchmark}


\begin{table}[H]
\caption{Примеры запросов из датасета SNIPS}
\label{tabular:timesandtenses}
\begin{center}
\begin{tabular}{ccc}
\hline
\textbf{Категория} & \textbf{Пример} \\ \hline
Get weather & What is the forecast at 12 am in Sudan. \\\hline
Play music  & Play some 1954 songs on my Itunes. \\\hline
Book restaurant  & What is the forecast at 12 am in Sudan. \\\hline
Search creative work  & Find me the Lace and Whiskey soundtrack. \\\hline
Add to playlist  & Add this artist to spring music. \\\hline
Rate book  & give this textbook a 5 out of 6 rating. \\\hline
Search movie schedule  & Where is Road to the Stage playing. \\\hline
\end{tabular}
\end{center}
\end{table}




Для подготовки тренировочных и тестовых данных на шведском языке мы использовали API сервиса Яндекс Переводчик \footnote{\url{https://translate.yandex.ru/developers}}. 

Перед использованием для обучения моделей текст проходил предобработку. Она состояла из нескольких этапов.  \par

\begin{enumerate}
\item Приведение всех букв к нижнему регистру
\item Замена всех пробельных символов на пробелы
\item Снятие всех ударений, умляутов и подобных знаков. Удаление всех не ascii символов
\item  Раскрытие всех сокращений (Например "You're --> you are")
\item Удаление всех специальных символов при помощи регулярных выражений
\item Удаление множественных пробелов
\end{enumerate}

После предобработки датасет содержал 13784 пары предложений. Медианная длинна запроса на английском языке -- 8 слов. Самые короткие запросы состоят из двух слов, например: "play pop". Самый длинный из 33 слов. Всего датасет включает 11282 уникальных английских слова, каждое слово в среднем встречается 10 раз. \par
Медианная длинна запроса на шведском языке также 8 слов. Уникальных слов на 14\% больше чем в английском -- 12852. Весь датасет включает по 119529 и 115914 слов в английском и шведском языках соответственно. 

\newpage
\section{Алгоритм}

\subsection{Статистический анализ}
Для лучшего понимания структуры данных и с целью развития интуиции для отбора моделей, в ходе работы был произведен небольшой статистический анализ набора данных. Некоторые рассмотренные ниже модели опираются на слово как на единицу информации и рассматривают предложения как неупорядоченные наборы слов т.н. "мешки слов". Было решено сравнить отдельные слова по некоторой величине отражающей ценность этого слова для классификации предложений.  
\subsubsection{Метод}
Для описания методики вычисления полезности слова для классификации введем некоторые обозначения. Пусть язык E содержит \(N_e\) уникальных слов. \(\{w_{1}, \dots ,w_{N_{e}}\}\) - всевозможные уникальные слова языка \(E\). Слова встречаются в предложениях, всего есть \(D\) предложений. \(\{ s_1, \dots , s_D \}\). Кроме того каждое предложение относится к тому или иному классу \(k  \in K = \{k1, \dots k_n\}\). Рассмотрим два индикатора. Индикатор принадлежности классу:
\[ I_k(s, k) =
  \begin{cases}
    1       & \quad \text{если предложение } s \text{ принадлежит классу } k\\
    0  & \quad \text{если предложение } s \text{ не принадлежит классу } k
  \end{cases}
\]
И индикатор вхождения слова в предложение:

\[ I_s(s, w) =
  \begin{cases}
    1       & \quad \text{если предложение } s \text{ содержит слово } w\\
    0  & \quad \text{если предложение } s \text{ не содержит слово } w
  \end{cases}
\]


Тогда введем следующие обозначения: \(c_{i,j}\) --  количество раз которое слово \(w_i\) встретилось в предложениях класса \(k_j\).
\[ 
c_{i,j} = \sum_{\substack{
   0<t<D
  }} 
    I_k(s_t,k_j) * I_s(s_t, w_i)
 \]
\(p_{i,j}\) - доля предложений с меткой \(k_j\) среди всех предложений со словом \(w_i\).
\[ 
p_{i,j} = \frac{c_{i,j}}{\sum_{\substack{
   0<t<D
  }} 
    I_s(s_t, w_i)
}
 \]
\(l_{i,j}\) = доля предложений в которых встречается слово \(w_i\)  среди предложений с меткой \(k_j\). 

\[ 
l_{i,j} = \frac{c_{i,j}}{\sum_{\substack{
   0<t<D
  }} 
    I_k(s_t, k_j)
}
 \]

Используя введенные обозначения запишем выражение для вычисления статистики \(U(w_i)\) отражающей условную полезность слова \(w_i\) для задачи классификации. 
\[
U(w_i) =  \sum_{\substack{ 0<j<n }} 
    [(p_{i,j} - \frac{1}{n})^2 * l_{i, j}] * \frac{n}{n-1}
\]

Значение величины \(U(w_i)\) стремится к нулю, при приближении распределения слова \(w_i\) по классам к случайному. 
\[
p_{i,j} \to \frac{1}{n}   \forall j \in \{1, \dots, n\} \ 
\implies 
u_i \to 0
\]
При этом, чем более вырождено распределение слова по классам и чем больше предложений класса содержат это слово, тем больше будет значение метрики. Если слово \(w_i\) встречается исключительно в предложениях класса \(k_j\) и при этом каждое предложение класса \(k_j\) содержит это слово, то значение метрики достигнет единицы. Это будет означать, что одно это слово позволяет безошибочно указать на принадлежность всех предложений содержащих его к конкретному классу \(k_j\). 

\subsubsection{Анализ}

Вычислив условную полезность для классификации для каждого слова на английском и шведском языке  мы построили рейтинг.  В таблице приведены 20 "наиболее полезных для классификации в рамках данной задачи" слов и их \(U\) величина. Некоторые слова, являющиеся прямым переводом друг друга, выделены цветом. 
Кроме того в таблице приведены значения:
\[
mplp = \max_{j=1 \dots n}  p_{i,j} \text{   (most popular label probability)}  
\]
\[
mpll = l_{i,j} | j = \argmax_{j=1 \dots n} p_{i,j} \text{   (most popular label load)} 
\]
\[
label = k_j | j = \argmax_{j=1 \dots n} p_{i,j}
\]

\begin{figure}[h]
\caption{Топ-20 слов по "полезности для классификации" в Английском и Шведском языках}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{stats.png}
\end{figure}

В первую очередь по результатам приведенным в таблице можно сказать что предложенная метрика  действительно обладает смыслом. Слова попавшие в таблицу похожи на слова, имеющие значение для классификации. Например слово "play"(играть) имеет высокое значение для распознавания  метки "PlayMusic" (включить музыку). Что вполне логично. 
\par
Также можно заметить, что слова имеющие один и тот же смысл не всегда находятся на одних и тех же позициях в разных языках. Так например слово "book" в английском языке находится ниже чем один из его переводов "boka" в шведском. Это связано с тем, что в английском языке слово "book" имеет несколько значений и может означать как "забронировать" так и "книга". И потому имеет меньшую полезность для классификации в этих классах. Аналогичная ситуация наблюдается со словом "table" (стол, таблица).

\subsection{Модели Векторов Предложений}
Как уже говорилось ранее, задачи обработки языка подразумевают необходимость представления слов и предложений на естественном языке в каком-либо удобном для обработки виде. Как правило это вектора. В нашей работе мы будем активно использовать представление предложений как векторов, то есть функцию \(J_s\) будем искать в виде:
\[J_s(s) = J_s(V_s(s)) \]
где \(V_s\) - преобразование предложения на шведском языке к вектору. 
 
Для того чтобы изучать перенос знаний модели для английского языка на другой, нужно сначала подобрать хорошую архитектуру, работающую в одном домене.  \par
В нашей работе мы пользовались несколькими разработанными методиками для представления предложений в виде векторов в некотором пространстве. \par

\subsubsection{FastText - average} 
Для самой простой методики построения векторов предложений мы опирались на готовую модель от Facebook -- fastText \cite{fasttextRepresentations}. Это реализация алгоритма CBOW для получения векторов слов с некоторыми дополнениями. Для обучения модели в качестве входных параметров Fasttext использует не только слова контекста, как в классической реализации \cite{w2v}, но и символьные n-граммы. То есть все подслова определенной длины. Итоговый вектор слова получается как сумма таких n-грамм. Эта методика была описана командой Facebook AI Research в статье \cite{fasttext} и показала свою эффективность в задачах поиска похожих слов и аналогий. Наличие информации о подсловах при обучении модели позволяет использовать ее для получения векторов слов для слов ранее не встречавшихся в словаре. 
\par
В ходе работы мы сами не обучали модель, а использовали опубликованные в открытом доступе предобученные вектора fastText. Эти данные распространяются по лицензии CC BY-SA 3.0, найти их можно по адресу \url{https://fasttext.cc/docs/en/crawl-vectors.html} . При обучении векторов использовалась архитектура CBOW с n-граммами длины 5, размером окна 5 и размером вектора 300. Для  обучения использовались тексты с Wikipedia и других открытых источников. \par
Для получения векторов предложений мы брали среднее значение всех векторов слов полученных из модели. Такой подход не учитывает порядок и взаимное расположение слов, зато работает очень быстро. 

\subsubsection{FastText - взвешенное среднее}

При усреднении всех векторов слов для получения вектора предложения информация содержащаяся в отдельных словах зашумляется. Многие слова не несут полезной для классификации смысловой нагрузки. Хотелось бы отличать такие слова и не использовать их для усреднения.  \par    
Мы протестировали еще одну методику получения векторов предложений на основе fastText. Для получения вектора предложения мы вычисляли взвешенное среднее всех слов входящих в него. В качестве веса использовалась предложенная нами в предыдущей главе величина полезности слова для классификации \(U(w)\).

\[
V_s(s) =  \frac{\sum_{\substack{
   w_i \in s
  }} 
    V_{fastText}(w_i) * U(w_i)
    }{\sum_{\substack{
   w_i \in s
  }} 
    U(w_i)
}
\]
При заранее вычисленных величинах \(U(w_i) \forall i \in [1 \dots N]\) такой подход также работает быстро, но позволяет снизить количество шума в векторе и тем самым поднять качество классификации. 

 \subsubsection{ELMo - среднее} 
 
В качестве альтернативы fastText мы рассмотрели другую модель для получения векторов предложений  -- ELMo  (Embeddings from Language Models)\cite{elmo}. Эта методика основана на использовании внутренних слоев обученной двунаправленной языковой LSTM сети для предсказания слов. На первом слое двунаправленной сети подаются вектора слов полученные из сверточной сети на символах. Затем следуют два biLSTM слоя. Итоговый вектор слова получается как линейная комбинация выходных значений на предыдущих трех слоях.  \par
В нашей работе, мы, так же как и в случае  с fastText, не обучали модель самостоятельно, а использовали предобученные.  Данные использованные в работе \cite{elmoRepresentations} \cite{elmoHost} доступны на сайте \url{https://github.com/HIT-SCIR/ELMoForManyLangs#downloads}. Данная модель была обучена на корпусе в 20 миллионов слов случайно набранном из Wikipedia и других открытых источников. Размерность получаемого вектора слова -- 1024.  \par
Данная модель, естественным образом использует информацию о расположении слов при вычислении векторов, так как включает в себя двунаправленные LSTM модули. Однако вычисления занимают несколько больше времени, в сравнении с fastText. \par
Как и в случае с fastText векторами, в качестве вектора предложения мы брали среднее значение векторов всех его слов.


\subsubsection{Universal Sentence Encoder} 

USE \cite{use} -- наиболее современная из трех рассмотренных технологий предложенная в апреле 2018 года исследовательской группой Google. Построение векторов предложений предложенное в статье основывается на использовании глубокой нейронной сети для преобразования отдельных векторов слов к вектору предложения. Данная сеть обучается на различных задачах обработки языка, что позволяет достичь существенных результатов в разных областях. Однако получение векторов требует существенных вычислительных затрат.   \par
В нашей работе мы использовали предобученную модель реализованную с использованием фреймворка TensorFlow \cite{tf} размещенную по адресу \url{https://tfhub.dev/google/universal-sentence-encoder/2}. Модель распространяется по лицензии CC BY 3.0. Размерность получаемых векторов предложений -- 512.
  \par\par

 Для оценки качества векторов предложений в контексте задачи распознавания намерения пользователя мы обучали однослойную нейронную сеть предсказывать вероятности  классов. Ошибка сети считалась с помощью отрицательной логарифмической функции правдоподобия. Для изменения весов использовался метод Adam \cite{adam}. Точность полученного классификатора измерялась как процент правильно предсказанных меток для тестовой выборки с использованием кросс-валидации для обоих языков. 

\subsection{Линейное преобразование}

Допустим у нас есть модель \(J_e\), которая по вектору запроса на английском языке хорошо предсказывает намерение \(K\). Тогда для решения задачи предсказания намерения для второго языка, достаточно построить преобразование векторов запроса со второго языка на английский и затем воспользоваться \(J_e\). Заметим, что такое преобразование не будет являться переводом, так как восстановление предложения по его вектору практически невозможно. \par
В качестве первого тестируемого метода поиска преобразования мы выбрали простую линейную модель. Функция \(L(V_s(s))\) искалась в виде \(L(V_s(s)) = V_s(s)A\). Для нахождения матрицы преобразования A решалась линейная система уравнений на тренировочном подмножестве данных. 
\[V_s A = V_e \]
Где \(V_s\) и \(V_e\) -- матрицы векторов предложений размерности \(N_v\) на \(N_{train}\). 
\(N_v\) -- Длина вектора предложения в  представлении \(V\), \(N_{train}\) -- количество примеров обучающей выборки

Так как \(N_{train}\) как правило сильно больше \(N_v\) система получается переопределенной. Под решением мы понимаем такую матрицу A, которая минимизирует Евклидово расстояние между столбцами в правой и левой частях выражения. Для нахождения  матрицы A используется метод наименьших квадратов. 


В нашем эксперименте, для выбора предсказанной метки приложения, мы находили расстояние от образа вектора \(V_s(s_i)\) до всех векторов \(V_e(e_j)\) из тестовой выборки кроме непосредственного перевода \(s_i\) и в качестве предсказанного класса брали класс предложения \(e_j\), расстояние до вектора которого было минимальным. 

\[
J_s(s_i) = 
J_e(\argmin_{
    \overset{j=1 \dots N_{train}}{j \neq i }
}
(\|L(V_s(s_i)) - V_e(e_j)\|^2))
\]

\subsection{Нейронная сеть}

Линейное преобразование -- сильно ограниченный в своей гибкости способ построения связи между векторами различных языков. Кроме того, усредняя вектора fastText для шведского мы теряем часть информации.  Помимо линейного преобразования мы также попробовали предсказывать USE вектор предложения на английском при помощи нейронной сети  с рекуррентными и сверточными слоями. В качестве входных параметров сеть получала матрицу из отдельных векторов для каждого шведского слова в предложении. Таким образом алгоритм имел возможность уловить информацию  о связи между словами и из взаимным расположением. \par  

Общая архитектура сети выглядела следующим  образом. К матрице из векторов слов применялись сверточные фильтры захватывающие по два, три или пять слов. Параллельно слова по одному подавались на вход рекуррентной нейронной сети с LSTM модулями в прямом и обратном порядке. Два полученных вектора скрытого состояния последнего слоя рекуррентной сети склеивались с векторами из максимальных значений по каждому фильтру. Затем следовал один полносвязный слой. При обучении перед полносвязным слоем 35\% случайных элементов занулялись для предотвращения переобучения. В результате мы получали вектор нужной нам размерности. \par 

\begin{figure}[ht]
\caption{Архитектура использованной в работе нейронной сети}
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{rnn-arc.png}
\end{figure}


Помимо LSTM модулей, мы также тестировали аналогичную архитектуру с GRU модулями в рекуррентной сети. \par
Для обучения мы на вход сети подавали матрицу из векторов слов fastText шведского предложения и подбирали веса для минимизации квадрата расстояния между выходом сети и USE вектором соответствующего английского предложения.  \par\newpage
Для предсказания метки класса мы пробовали два метода. 
\begin{enumerate}
    \item  Как и в случае с линейным преобразованием, предсказанным классом шведского предложения считался класс английского предложения из тестовой выборки, чей USE вектор ближе всего к выходу сети.
    \item Вектор полученный рекуррентной сетью подавался на вход обученному на тренировочных USE векторах линейному классификатору. Метка предсказанная классификатором считалась меткой шведского предложения. 
\end{enumerate}


\newpage
\section{Результаты}


\subsection{Модели Векторов Предложений}

В данной таблице представлены точности классификации запросов пользователя по классам в зависимости от используемой технологии получения векторов предложений. 

\begin{table}[H]
\caption{Точность классификации  для моделей основанных на векторах предложений для разных языков}
\label{tabular:single_lang_results}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Модель} & \textbf{Английский} & \textbf{Шведский} \\
\hline
FastText-avg & 91.9\% & 88.3\% \\
\hline
FastText-uw-avg & 96.2\% & 94.2\% \\
\hline
ELMo-avg & 97.7\% & 96.0\% \\
\hline
USE & 96.8\% & -- \\
\hline

\end{tabular}
\end{center}
\end{table}

\subsection{Линейное Преобразование}

В ходе работы было протестировано 6 различных пар представлений предложений на Английском и Шведском языках. Для каждой пары мы вычисляли матрицу преобразования векторов с Шведского языка на Английский, и пользовались ей для предсказания класса шведского предложения. \par
В таблице представлен средний процент правильно предсказанных меток и среднеквадратичная ошибка после кросс-валидации в зависимости от используемых векторов предложений для разных языков.

\begin{table}[H]
\caption{Точность модели с линейным преобразованием векторов предложений}
\label{tabular:linear_results}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Шведский} & \textbf{Английский} & \textbf{Точность} & \textbf{Ошибка} \\ \hline
FastText-avg & ELMo-avg & 86.2& 0.00745 \\ \hline
FastText-avg & USE & 89.6\% & 0.00083 \\ \hline
FastText-uw-avg & ELMo-avg & 88.8\%& 0.00881 \\ \hline
FastText-uw-avg & USE & 91.2\%& 0.00095 \\ \hline
ELMo-avg & ELMo-avg & 89.1\%& 0.00682 \\ \hline
ELMo-avg & USE & 91.5\%& 0.00081 \\ \hline

\end{tabular}
\end{center}
\end{table}

\subsection{Нейронная сеть}

Описанная выше архитектура по векторам слов fastText на шведском языке предсказывала вектора USE для английского. Было протестировано два метода предсказания класса. С использованием "ближайшего соседа", и с использованием USE классификатора. В таблице приведен процент правильно предсказанных меток шведских предложений и среднеквадратичная ошибка между предсказанными и реальными векторами USE. 

\begin{table}[H]
\caption{Точность модели с нейронной сетью.}
\label{tabular:RNN-results}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Рекуррентные модули} &\textbf{Метод предсказания} & \textbf{Точность}& \textbf{Ошибка} \\ \hline
GRU & Ближайший сосед & 94.1\% & 0.00068\\ \hline
GRU & USE классификатор & 96.2\%& 0.00068 \\ \hline
LSTM & Ближайший сосед & 93.7\% & 0.00069\\ \hline
LSTM & USE классификатор & 96.1\%& 0.00069 \\ \hline
\end{tabular}
\end{center}
\end{table}

Можно заметить, что полученная архитектура предсказывает метку предложения по fastText векторам  лучше чем рассмотренный ранее  линейный классификатор не использующий знания о связанных английских предложениях и их USE векторах. \par
Для сравнения мы так же пробовали предсказывать метку класса по USE вектору на основании метки ближайшего соседа в пространстве USE векторов без каких-либо преобразований. На всем датасете мы получили точность в \textbf{94.3\%}

\newpage
\section*{Выводы}
В данной работе ставилась задача оценить эффективность переноса опыта модели с одного языка на другой на примере задачи предсказания намерения. В ходе работы было построено несколько моделей, использующих этот подход. Протестированные алгоритмы показали точность сравнимую с точностью простых моделей работающих на английском языке для шведского. Это позволяет сделать вывод об эффективности предложенных методов и возможности их использования в реальных задачах. \par
Было замечено что GRU модули в нейронной сети работают незначительно лучше чем LSTM. Вероятно более простые архитектуры также позволят достичь сравнимых результатов. Имеет смысл продолжить эксперименты для поиска максимально простой модели, достаточной для полного переноса знаний. \par
Кроме того в работе был предложен метод оценки полезности слов для классификации в конкретной задаче. Результаты этого метода были применены для улучшения результатов классификации с использованием одного из методов. 

\newpage
\section*{Заключение}

В работе было рассмотрено несколько моделей построения векторов предложений и изучены особенности  различных подходов. \par
Был предложен метод ранжирования слов в предложении  по полезности для классификации и показано что такое ранжирование может быть использовано для улучшения некоторых моделей построения векторов предложений.  \par
Было рассмотрено две методики переноса знаний с одного языка на другой. С использованием линейного преобразования и с использованием нейронной сети с рекуррентными и сверточными слоями. \par
Обе методики были протестированы в разных конфигурациях на разных входных данных. Были получены результаты и показано что перенос знаний при помощи адаптации векторов слов нейронной сетью позволяет увеличить точность по сравнению с простой моделью работающей в одном языке. 
\newpage



\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{bi} % Entries are in the "bi.bib" fill
\end{document}


