# Метод межязыковой адаптации диалоговых систем
*Коробков Никита*

### Введение

В последние годы все больше внимания уделяется решению различных задач с применением машинного обучения и нейронных сетей. Одним из наиболее интересных направлений в области применения современных нейросетевых методов являются диалоговые системы.  
Диалоговая система это набор программ и алгоритмов позволяющих человеку вести диалог с программой в манере свойственной человеческой. Иногда диалоговые системы называют разговорным искуственным интелектом или "чат-ботом". 
Задача построения подобных программ является актуальной для промышенной области потому, что решение задач пользователей путем диалога с агентом поддержки всегда было и будет оставаться наиболее простым и эффективным методом. Так как вербальное общение наиболее естественный для человека способ коммуникации. Построение хорошей и надежной диалоговой системы заменяющей работников колл центра позволило бы существенно сократить затраты рессурсов.  
Диалоговые системы можно разделить на системы общего назначения и задачеориентированные. Чем более узкую задачу призвана решать система, тем проще она может быть устроена. В самом тривиальном случае cистема может просто возвращать заранее известный ответ. Например текущее время.  
Мы хотели бы построить систему решающую более общую задачу. Для этого в нужно сначала выяснить детально чего именно хочет пользователь (задать вопрос и получить ответ). В самой тривиальной реализации этот сценарий может выглядеть как простой выбор задачи из списка. С применением такого подхода в промышленности можно столкнуться уже сегодня. Позвонив в банк или авиакомпанию можно часто услышать робота, который предложит нажать разные кнопки в зависимости от цели звонка ("Для проверки балланса нажмите 1, для уточнения статуса заявки нажмите 2" и.т.п.). Этот подход хорош тем, что не требует от системы никакой интеллектуальности и работает очень надежно. Однако занимает много пользовательского времени и зачастую нервирует. В идеальном случае мы хотели бы получив запрос в виде предложения на натуральном языке, например "Какая погода сейчас на улице?" сразу распознать намерение пользователя.  
Это одна из подзадач в построении диалоговых систем, на которой хотелось бы сконцентрировать наше внимание в этой работе. 
Данная задача является довольно актуальной и стоит уже давно. Так что для ее решения было предложенно множество методик. Подобнее они будут рассмотрены в разделе "Обзор литературы". Большинство методов ориентированно на работу с английским языком. В основном потому, что для английского собранно наиболее большое колличество данных. Кроме того английский просто считается языком по умолчанию в научной среде.  
В то время как для английского языка достигнуты внушительные результаты, ситуация с другими языками обстоит несколько иначе. Представленные модели в большинстве своем обучены на коллосальном объеме размеченных данных. Таких данных не существует для более редких языков. По этому простое перенесение достигнутых результатов путем обучения идентичной модели на другом языке не представляется возможным.  
Тем не менее, почти для всех мировых языков существуют полные словари. Пользуясь общими знаниями о связи двух языков (словари, параллельные тексты) можно обобщить знания одной модели на другой язык не используя размеченных данных для второго языка совсем (либо используя совсем немного). Данный подход в литературе носит название Transfer Learning.
Целью данной работы будет построение модели для извлечения намерения из предложения на **шведском** языке при помощи переноса знаний накопленных обученной на английском языке моделью. Выработанную методику можно будет использовать для построения моделей приблизительно такой же точности для любого другого языка для которого существует словарь перевода его слов на английский. 

### Постановка задачи

Пусть существует множество комманд на английском языке \\(E\\) и конечное множество намерений \\(K\\)

\\[ 
    K = \{k_1,k_2,...,k_n\}
\\]
Каждой команде из \\(E\\) однозначно соответствует элемент множества \\(K\\). Соответствие обозначим \\(I_e\\)
\\[ 
    I_e : E \to K
\\]
Тренировочные данные состоят из множеств \\(E, K\\) и соответствия \\(I_e\\).  При этом существует так же функци-переводчик \\(T\\), которая каждой команде на английском языке ставит в соответствие команду на шведском языке. Множество комманд на шведском языке обозначим \\(S\\)
\\[ 
    T : E \to S
\\]
Целью данной работы будет получение функции \(I_s : S \to K \) сопоставляющей любой команде на шведском языкее намерение. При этом должны выполняться два условия. 

1.  Намерение должно совпадать с намерением перевода шведской команды на английский язык. 

	\begin{equation} \label{eq:reverse_is}
	\forall s \in S \quad I_s(s) = I_e(T^{-1}(s)) 
	\end{equation}
    \begin{equation} \label{eq:straight_is}
	\forall e \in E \quad I_s(T(e)) = I_e(e)
	\end{equation}

2. Функция \(I_s\) не должна зависеть от функции \(T\)

Второе условие является определяющим для данной задачи. Если бы мы могли использовать функцию \(T\) в \(I_s\) то можно было бы просто определить \(I_s\) как в \eqref{eq:reverse_is} и остановиться на этом.  
Но это невозможно, так как перевод (вычисление \(T^{-1}\)) это слишком дорогостоящая операция. Мы бы хотели получить функцию которая была бы достаточно легковычислимой для использования в мобильных приложениях. Поэтому вместо прямого перевода комманды со шведского языка на английский и последующего применения существующих алгоритмов мы постараемся выделить какие-то ключевые атрибуты комманды на шведском языке и использовать их для распознавания намерения. \par
В силу требования 2 мы едва ли сможем удовлетворить требование 1 полностью. Вместо этого попытаемся построить систему, которая на тренировочных данных сможет максимально часто предсказывать намерения пользователя правильно. В качестве метрики качества предсказаний будем исползовать долю комманд в тестовой выборке, по которой система приняла правильное решение. \par

### Обзор литературы

Задача построения диалоговых систем лежит в области автоматического анализа текстовых данных. Одним из главных вопросов применения нейронных сетей в данной области является эффективное представление слов в памяти компьютера. При анализе текстов мы бы хотели заменять слова на вектора как-то отражающие семантический смысл слова.  
Наиболее распространенный метод получения векторов слов описан в работе \cite{w2v}
В данном подходе обучающий текстовый корпус просматривается окном ширины 2h + 1 слов, и для каждого окна однослойная нейронная сеть предсказывает центральное слово окна w(t) по окружающим w(t + i), i ∈ [−h, h] или
наоборот. Эти архитектуры называются Continuous Bag-of-words и Skipgram соответственно. Минимизируя ошибку предсказания, нейронная сеть строит проекцию слов в векторное пространство заранее определенной размерности. При достижении заданной точности предсказания или определенного
числа эпох, алгоритм генерирует словарь с векторными представлениями для слов из обучающего корпуса.

\begin{figure}[h]
\caption{Архитектуры нейронных сетей, представленные в \cite{w2v} для окна ширины
h = 5}
\centering
\includegraphics{word2vec.png}
\end{figure}

Данный подход позволяет получить вектора обладающие свойством семантической близости. Мы можем надеяться что слова обладающие схожим смыслом будут находится рядом в построенном векторном пространстве.  
После того как методы описанные в \cite{w2v} показали свою эффективность в ряде задач обработки текстов \cite{LM}. Было разработано и предложенно несколько похожих методов построения векторов слов \cite{elmo}, \cite{glove}

В данный момент большинство методик обработки естественного языка так или иначе использует вектора слов. 

В нашей работе мы использовали дополненную реализацию оригинального word to vec "fasttext" \cite{fasttext}. В отличие от оригинальной архитектуры этот подход помимо слов контекста использует части слов для обучения, что позволяет предсказывать вектора слов для слов отсутствующих в тренировочной коллекции. 

Для работы с последовательностями слов (предложениями) часто применяются рекурентные нейронные сети описанные в статьях \cite{rnn} \cite{rnn2}. Воизбежание проблемы затухающих градиентов при обучении, используют LSTM архитектуру \cite{lstm}.

В статье Attention is all you need \cite{attention} группа исследователей из Google описывает принципиально новый подход к обработке последовательной текстовой информации и в частности к переводам. Вместо классической архитектуры рекурентных нейронных сетей с использованием LSTM или GRU модулей автор испозьзует так называемый механизм внимания, Который позволяет более качественно представлять информацию содержащуюся в предложениях на этапе кодирования. Такую сеть так же называют "Трансформер" из за гибкости внутренней структуры, позволяющей получать разную информацию о кодируемом предложении в зависимости от запроса. Многие современные автоматические переводчики пользуются этой технологией. В нашей работе мы пользовались готовой системой автоматического перевода от  Яндекс. Примерное описание механизмов работы их переводчика доступно в статье \cite{ytranslate}

Идея трансформер сетей развивается в статье \cite{bert}. Авторы предлагают тренировать многослойную модель из трансформер модулей на задаче определения связности предложений и предсказания пропущенного слова. Полученная модель показывает исключительные результаты после дообучения на ряде конкретных задач. 

Задача предсказания намерения из фиксированного множества может быть сформулированная как задача классификации предложений. Интересный подход к этой задаче с использованием сверточных сетей предложен в статье \cite{deepPavlovCNN}.
### Обработка данных

В нашей работе в качестве тренировочных и тестовых данных мы использовали датасет собранный компанией SNIPS \cite{snips}. 
Данные включают в себя более 13000 запросов пользователей на английском языке. Каждый запрос относится к одной из семи категорий по намерению пользователя. 
 
Представленные намерения:
\begin{itemize}
  \item Get weather (Посмотреть погоду - 2000 записей)
  \item Play music (Включить музыку - 2000 записей)
  \item Book restaurant (Забронировать ресторан - 1973 записей)
  \item Search creative work (Посик произведений - 1954 записей)
  \item Add to playlist (Добавить в плейлист - 1942 записей) 
  \item Rate book (Поставить оценку книге - 1956 записей)
  \item Search movie schedule (Расписание сеансов кино - 1959 записей)
\end{itemize}

Данные предоставленны компанией SNIPS по лицензии Creative Commons Zero v1.0 Universal и доступны для скачивания по ссылке \url{https://github.com/snipsco/nlu-benchmark}

\begin{table}[H]
\caption{Примеры запросов из датасета SNIPS}
\label{tabular:timesandtenses}
\begin{center}
\begin{tabular}{ccc}
\textbf{Категория} & \textbf{Пример} \\
Get weather & What is the forecast at 12 am in Sudan. \\
Play music  & Play some 1954 songs on my Itunes. \\
Book restaurant  & What is the forecast at 12 am in Sudan. \\
Search creative work  & Find me the Lace and Whiskey soundtrack. \\
Add to playlist  & Add this artist to spring music. \\
Rate book  & give this textbook a 5 out of 6 rating. \\
Search movie schedule  & Where is Road to the Stage playing. \\
\end{tabular}
\end{center}
\end{table}
Для подготовки тренировочных и тестовых данных на шведском языке мы использовали API сервиса Яндекс Переводчик \footnote{\url{https://translate.yandex.ru/developers}}. 

Перед использованием для обучения моделей текст проходил предобработку. Она состояла из нескольких этапов. 
1. Приведение всех букв к нижнему регистру
1. Замена всех пробельных символов на пробелы
1. Снятие всех ударений, умлаутов и подобных знаков. Удаление всех не ascii символов
1. Раскрытие всех сокращений (Например "You're --> you are")
1. Удаление всех специальных символов при помощи регулярных выражений
1. Удаление множественных пробелов

После предобработки датасет содержал 13784 пары предложений. Медианная длинна запроса на английском языке -- 8 слов. Самые короткие запросы состоят из двух слов, например: "play pop". Самый длинный из 33 слов. Всего датасет включает 11282 уникальных английских слова, каждое слово в среднем встречается 10 раз. 
Медианная длинна запроса на шведском языке также 8 слов. Уникальных слов на 14% больше -- 12852. Весь датасет включает по 119529 и 115914 слов в английском и шведском языках соответственно. 

### Алгоритм

#### Статистический анализ

Для лучшего понимания структуры данных и с целью развития интуиции для отбора моделей, в ходе работы был произведен небольшой анализ статистический анализ набора данных. Некоторые рассмотренные ниже модели опираются на слово как на единицу информации и рассматривают предложения как неупорядоченные наборы слов т.н. "мешки слов". Было решено сравнить отдельные слова по некоторой величине отражающей ценность этого слова для классификации предложений.  
##### Метод (\subsubsection{Метод}
Для описания методики вычисления полезности слова для классификации введем некоторые обозначения. Пусть язык E содержит N_e уникальных слов. w1 ... wN_ - всевозможные уникальные слова языка E. Слова встречаются в предложениях, всего есть D предложений. s1 .. sD. Кроме того каждое предложение относится к тому или иному классу K = k1 .. k2. Введем два индикатора. Индикатор принадлежности классу I_k(s, k) = 1 если предложение s принадлежит классу k  и ноль в противном случае. И индикатор вхождения слова в преложение I_s(s, w) ..
Тогда введем следующие обозначения -- с_ik = колличество раз которое слово i встретилось в предложениях класса k. p_ik = доля предложений с  меткой k среди всех предложений со словом i. l_ik = доля предложений в которых встречается слово i  среди предложений с меткой k. 

Используя введенные обозначения запишем выражение для вычисления статистики U отражающей условную полезность слова для задачи классификации. 

U = 

Значение величины Ui стремится к нулю, при приближении распределения слова wi по классам к случайному. 
pi --> 1/n => ui --> 0
При этом, чем более вырождено распределение слова по классам и чем больше предложений класса содержат это слово, тем больше будет значение метрики. Если слово wi встречается исключительно в предложениях класса k_j и при этом каждое преложение класса k_j содержит это слово, то значение метрики достигнет единицы. Это будет означать, что одно это слово позволяет безошибочно указать на принадлежность всех предложений содержащих его к конкретрому классу. 
Экспериментальные данные подтверждают полезность приведенной метрики. 

#### Подбор модели векторов слов
Как уже говорилось ранее, задачи обработки языка подразумевают необходимость представления слов и предложений на естественном языке в каком-либо удобном для обработки виде. Как правило это вектора. В нашей работе мы будем активно использовать представление преложений как векторов, то есть функцию J_s будем искать в виде:
J_s(s) = J(V_s(s)) 
где V_s - преобразование предложения на шведском языке к вектору. 
 
Для того чтобы изучать перенос знаний модели для английского языка на шведский, нужно сначала подобрать хорошую архитектуру, работающую в одном домене.  \par
В нашей работе мы пользовались несколькими разработанными методиками для представления предложений в виде векторов в некотором пространстве. \par

\subsubsection{fastText - среднее} 
Для самой простой методики построения векторов предложений мы опирались на готовую модель от Facebook -- fastText \cite{fasttextRepresentations}. Это реализация агоритма CBOW для получения векторов слов с некоторыми дополнениями. Для обучения модели в качестве входных параметров Fasttext использует не только слова контекста, как в классической реклизации \cite{w2v}, но и символьные n-граммы. Тоесть все подслова определенных длинн. Итоговый вектор слова получается как сумма таких n-грамм. Эта методика была описана командой Facebook AI Research в статье \cite{fasttext} и показала свою эффективность в задачах поиска похожих слов и аналогий. Наличие информации о подсловах при обучении модели позволяет использовать ее для получения векторов слов для слов ранее не встечавшихся в словаре. 
\par
В ходе работы мы сами не обучали модель, а исползовали опубликованные в открытом доступе предобученные вектора fastText. Эти данные распространяются по лицензии CC BY-SA 3.0, найти их можно по адресу{}. При обучении векторов использовалась архитектура CBOW с n-граммами длинны 5, размером окна 5 и размером вектора 300. Для  обучения использовались тексты с Wikipedia и других открытых источников. \par
Для получения векторов предложений мы брали среднее значение всех векторов слов полученных из модели. Такой подход не учитывает порядок и взаимное расположение слов, зато работает очень быстро. 

\subsubsection{fastText - взвешенное среднее}

При усреднении всех векторов слов для получения вектора предложения информация содержащаяся в отдельных словах зашумляется. Многие слова не несут полезной для классификации смысловой нагрузки. Хотелось бы отличать такие слова и не использовать их для усреднения.  \par    
Мы протестировали еще одну методику получения векторов предложений на основе fastText. Для получения вектора предложения мы вычисляли взвешенное среднее всех слов входящих в него. В качестве веса использовалась предложенная нами в предыдущей главе величина полезности слова для классификации \(U(w)\).

\[
V_s(s) =  \frac{\sum_{\substack{
   w_i \in s
  }} 
    V_{fastText}(w_i) * U(w_i)
    }{\sum_{\substack{
   w_i \in s
  }} 
    U(w_i)
}
\]
При заранее вычисленных величинах \(U(w_i) \forall i \in [1 \dots N]\) такой подход так же работает быстро, но позволяет снизить колличество шума в векторе и тем самым поднять качество классификации. 



 \subsubsection{ELMo - среднее} 
 
В качестве альтернативы fastText мы рассмотрели другую модель для получения векторов предложений  -- ELMo  (Embeddings from Language Models)\cite{elmo}. Эта методика основана на использовании внутренних слоев обученной двунаправленной языковой LSTM сети для предсказания слов. На первом слое двунаправленной сети подаются вектора слов полученные из сверточной сети на символах. Затем следуют два biLSTM слоя. Итоговый вектор слова получается как линейная комбинация выходных значений на предыдущих трех слоях.  \par
В нашей работе, мы, так же как и в случае  с fastText, не обучали модель самостоятельно, а использовали предобученные.  Данные использованные в работе \cite{elmoRepresentations} \cite{elmoHost} доступны на сайте \url{https://github.com/HIT-SCIR/ELMoForManyLangs#downloads}. Данная модель была обучена на корпусе в 20 миллионов слов случайно набранном из Wikipedia и других открытых источников. Размерность получаемого вектора слова -- 1024.  \par
Данная модель, естественным образом использует информацию о расположении слов при вычислении векторов, так как включает в себя двунаправленные LSTM модули. Однако вычисления занимают несколько больше времени, в сравнении с fastText. \par
Как и в случае с fastText векторами, в качестве вектора предложенния мы брали среднее значение векторов всех его слов.

\subsubsection{Universal Sentence Encoder} 

USE \cite{use} -- наиболее современная из трех рассмотренных технологий предложенная в апреле 2018 года исследовательской группой Google. Построение векторов предложений предложенное в статье основывается на использовании глубокой нейронной сети для преобразования отдельных векторов слов к вектору предложения. Данная сеть обучается на различных задачах обработки языка, что позволяет достичь существенных результатов в разных областях. Однако получение векторов требует существенных вычислительных затрат.   \par
В нашей работе мы использовали предобученную модель реализованную с использованием фреймворка TensorFlow \cite{tf} размещенную по адресу \url{https://tfhub.dev/google/universal-sentence-encoder/2}. Модель распространяется по лицензии CC BY 3.0. Размерность получаемых векторов предложений -- 512.
  
  
  Для оценки качества векторов предложений в контексте задачи распознавания намерения пользователя мы обучали однослойную нейронную сеть предсказывать вероятности  классов. Ошибка сети считалась с помощью отрицательной логарифмической функции правдоподобия. Для изменения весов использовался метод Adam \cite{adam}. Точность полученного классификатора измерялась как процент правильно предсказанных меток для тестовой выборки с использованием кросс-валидации для обоих языков. 
 
 //TODO: Поменять норму на косинусную
 //TODO: Нормализовать вектора перед взвешиванием в FT
 
  
#### Линейное преобразование

Допустим у нас есть модель \(J_e\), которая по вектору запроса на английском языке хорошо предсказывает намерение \(K\). Тогда для решения задачи предсказания намерения для второго языка, достаточно построить преобразование векторов запроса со второго языка на английский и затем воспользоваться \(J_e\). Заметим, что такое преобразование не будет являться переводом, так как восстановление предложения по его вектору практически невозможно. \par
В качестве первого тестируемого метода поиска преобразования мы выбрали простую линейную модель. Функция \(L(V_s(s))\) искалась в виде \(L(V_s(s)) = V_s(s)A\). Для нахождения матрицы преобразования A решалась линейная система уравнений на тренировочном подмножестве данных. 
\[V_s A = V_e \]
Где \(V_s\) и \(V_e\) -- матрицы векторов предложений размерности \(N_{train}\) на \(N_v\). 
\(N_v\) -- Длинна вектора предложения в  представлении \(V\), \(N_{train}\) -- колличество примеров обучающей выборки

Так как \(N_{train}\) как правило сильно больше \(N_v\) система получается переопределенной. Под решением мы понимаем такую матрицу A, которая минимизирует Евклидово расстояние между столбцами в правой и левой частях выражения. Для нахождения  матрицы A используется метод наименьших квадратов. 


В нашем эксперименте, для выбора предсказанной метки преложения, мы находили расстояние от образа вектора \(V_s(s_i)\) до всех векторов \(V_e(e_j)\) из тестовой выборки кроме непосредственного первода \(s_i\) и в качестве предсказанного класса брали класс предложения \(e_j\), расстояние до вектора которого было минимальным. 

\[
J_s(s_i) = 
J_e(\argmin_{
    \overset{j=1 \dots N_{train}}{j \neq i }
}
(\|L(V_s(s_i)) - V_e(e_j)\|^2))
\]
####  Нейронная сеть

Линейное преобразование -- сильно ограниченный в своей гибкости способ построения свзяи между векторами различных языков. Кроме того, усредняя вектора fastText для шведского мы теряем часть информации.  Помимо линейного  преобразования мы также попробовали предсказывать USE вектор предложения на английском при помощи  нейронной сети  с рекурентными и сверточными слоями. В качестве входных параметров сеть получала матрицу из отдельных векторов для каждого шведского слова в предложении. Таким образом алгоритм имел возможность уловить информацию  о связи между словами и из взаимным расположением. \par  

Общая архитектура сети выглядела следующим  образом. К матрице из векторов слов применялись сверточные фильтры захватывающие по два, три или пять слов. Параллельно слова по одному подавались на вход рекурентной нейронной сети с LSTM модулями в прямом и обратном порядке. Два полученных вектора скрытого состония последнего слоя рекурентной сети конкатенировались с векторами из максимальных значений по каждому фильтру. Затем следовал один полносвязный слой. При обучении перед полносвязным слоем 35% случайных элементов занулялись для предотвращения переобучения. В результате мы получали вектор нужной нам размерности. \par Помимо LSTM модулей, мы также тестировали аналогичную архитектуру с GRU модулями в рекурентной сети. Это позволило немного ускорить обучение модели, несущественно снизив качество преобразования. \par
Для обучения мы на вход сети подавали матрицу из векторов слов fastText шведского предложения и подбирали веса для минимизации квадрата расстояния между выходом сети и USE вектором соответствующего английского предложения.  \par
Для предсказания метки класса мы пробовали два метода. 
1. Как и в случае с линейным преобразованием, предсказанным классом шведского предложения считался класс английского предложения из тестовой выборки, чей USE вектор ближе всего к выходу сети.
2. Вектор полученный рекурентной сетью подавался на вход обученному на тренировочных USE векторах линейному классификатору. Метка предсказанная классификатором считалась меткой шведского предложения. 

### Результаты


Линейный классификатор
| Модель                | EN   | SV   | 
|-----------------------|------|------|
| fasttext - average    | 91.9 | 88.3 |
| fasttext - uw-avg     | 96.2 | 94.2 |
| fasttext - avg-km(50) | 72.8 | 61.2 |
| ELMo - average        | 97.7 | 96.0 |
| USE                   | 96.8 |      |



Ближайший вектор
| Модель                | EN   | SV   | 
|-----------------------|------|------|
| fasttext - average    | 85.5 | 81.0 |
| fasttext - uw-avg     | 92.1 | 89.9 |
| fasttext - avg-km(50) |  |  |
| ELMo - average        | 91.4 | 89.9 |
| USE                   | 92.2 | ---  |

Трансфер W2V-SV(300)  --> USE-EN(512) Линейное преобразование + модель от USE 71%
Трансфер ELMO-SV(1024)  --> USE-EN(512) Линейное преобразование + модель от USE 87%

#### Линейное преобразование
В ходе работы было протестированно 4 различных пары представлений предложений на Английском и Шведском языках. Для каждой пары мы вычисляли матрицу преобразования векторов с Шведского языка на Английский, и пользовались ей для предсказания класса шведского предложения. \par
В таблице представлен средний процент правильно предсказанных меток после кросс-валидации в зависимости от используемых векторов предложений для разных языков. 

| Линейный трансфер  | elmo-en   | use-en   |
|--------------------|-----------|----------|
| fasttext-sv        | 86.2      | 89.6     |  
| fasttext-uw-sv     | 88.8      | 91.2     |  
| elmo-sv            | 89.1      | 91.5     | 

### Хитрая RNN модель

Описанная выше архитектура по векторам слов fastText на шведском языке предсказывала вектора USE для английского. Было протестированно два метода предсказания класса. С использованием "ближайшего соседа", и с использованнием USE классификатора. В таблице приведен процент правильно предсказанных меток шведских предложений. 

с gru модулем 300 итераций -- 91.8 на ближайших векторах, 95.2 на предобученной линейной сети USE 

c lstm модулем 500 итераций -- 93.7 ? на ближайших векторах 96.1 на предобученном USE. 




### Выводы
### Заключение

