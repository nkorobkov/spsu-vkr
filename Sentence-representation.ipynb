{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# use this library https://github.com/facebookresearch/fastText/tree/master/python\n",
    "import fastText\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "with open('data/snips_processed/snips_train.csv', 'r') as f:\n",
    "    reader = csv.reader(x.replace('\\0', '') for x in f)\n",
    "    for line in reader:\n",
    "        trainset.append(line)\n",
    "trainset = np.array(trainset)\n",
    "\n",
    "testset = []\n",
    "with open('data/snips_processed/snips_test.csv', 'r') as f:\n",
    "    reader = csv.reader(x.replace('\\0', '') for x in f)\n",
    "    for line in reader:\n",
    "        testset.append(line)\n",
    "testset = np.array(testset)\n",
    "\n",
    "labels = list(set(trainset[:,0]))\n",
    "lab2id = {}\n",
    "id2lab = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    lab2id[labels[i]] = i\n",
    "    id2lab[i] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_model = fastText.load_model('data/cc.sv.300.bin')\n",
    "en_model = fastText.load_model('data/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_VEC = np.zeros((1, 300))\n",
    "EOS_VEC = np.zeros((1, 300))\n",
    "SOS_VEC[0,0] = 1\n",
    "EOS_VEC[0,1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vecs(sentence, model):\n",
    "    result = [SOS_VEC]\n",
    "    sentence = sentence.strip()\n",
    "    for word in sentence:\n",
    "        result.append([model.get_word_vector(word.lower())])\n",
    "    result.append(EOS_VEC)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pair(label, sentence, model):\n",
    "    return (lab2id[label]),(sentence2vecs(sentence, model))\n",
    "\n",
    "def prepare_pairs(data, lang = 'en'):\n",
    "    if lang == 'en':\n",
    "        model = en_model\n",
    "        slab = 1\n",
    "    elif lang == 'sv':\n",
    "        model = sv_model\n",
    "        slab = 2\n",
    "    else:\n",
    "        raise RuntimeError('lang is not supported')\n",
    "    labels = []\n",
    "    vectors = []\n",
    "    for sample in data:\n",
    "        l, v = prepare_pair(sample[0], sample[slab], model)\n",
    "        labels.append(l)\n",
    "        vectors.append(v)\n",
    "        \n",
    "    return labels, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prepare_pairs(trainset, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size=300):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "en_enc = EncoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_sent_vec(sentence_as_vecs, enc):\n",
    "    hidden = enc.initHidden()\n",
    "    for v in sentence_as_vecs:\n",
    "        input = torch.tensor([v]).float()\n",
    "        out, hidden = enc.forward(input, hidden)\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, in_size = 300, out_size = 10):\n",
    "        super(Baseline, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(300, 10)\n",
    "        self.out = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.W(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, en_optimizer,net_optimizer, labels, vectors, enc):\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    vectors = torch.cat([comp_sent_vec(x,enc) for x in vectors], 0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    model_out = model.forward(vectors)\n",
    "    loss += criterion(model_out[:,0], labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    en_optimizer.step()\n",
    "    net_optimizer.step()\n",
    "    \n",
    "    return loss.item()/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, labels, vectors, enc):\n",
    "    with torch.no_grad():\n",
    "        vectors = torch.cat([comp_sent_vec(x,enc) for x in vectors], 0)\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "        model_out = model.forward(vectors)\n",
    "        right = 0\n",
    "        \n",
    "        for i  in range(len(model_out)):\n",
    "            k, v = model_out[i].topk(1)\n",
    "            predicted, true = v.item(), labels[i].item()\n",
    "            if predicted == true:\n",
    "                right +=1\n",
    "\n",
    "                \n",
    "        loss = criterion(model_out[:,0], labels)\n",
    "        return loss.item(), right/len(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  5, 24682 sec. train loss: 0.0001263, eval loss: 1.0497, acc = 0.851, train_acc = 0.856\n",
      "# 10, 55418 sec. train loss: 0.0000606, eval loss: 0.6883, acc = 0.851, train_acc = 0.856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c691e78fd9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f70c722a0e8f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, en_optimizer, net_optimizer, labels, vectors, enc)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0men_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnet_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/virtualenvs/main/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/virtualenvs/main/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Baseline()\n",
    "enc_optimizer = torch.optim.Adam(en_enc.parameters())\n",
    "net_optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = torch.nn.NLLLoss()\n",
    "labs, vecs = prepare_pairs(trainset, lang = 'en')\n",
    "labst, vecst = prepare_pairs(testset, lang = 'en')\n",
    "t =  time.time()\n",
    "\n",
    "for i in range(1,1000):\n",
    "    loss = train(net, criterion, enc_optimizer,net_optimizer, labs, vecs, en_enc)\n",
    "    if not i% 5:\n",
    "        eval_loss, acc = evaluate(net, labst, vecst, en_enc)\n",
    "        _, train_acc = evaluate(net, labs, vecs, en_enc)\n",
    "        print('#{:3d}, {:5d} sec. train loss: {:.7f}, eval loss: {:.4f}, acc = {:.3f}, train_acc = {:.3f}'.format(i, int(time.time() - t), loss, eval_loss, acc, train_acc))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6516625881195068, 0.8510255487585462)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(net, labst, vecst, en_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  68'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:4d}'.format(int(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
