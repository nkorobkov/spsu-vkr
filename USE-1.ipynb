{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import csv, random, time\n",
    "\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "with open('data/snips_processed/snips.csv', 'r') as f:\n",
    "    reader = csv.reader(x.replace('\\0', '') for x in f)\n",
    "    for line in reader:\n",
    "        dataset.append(line)\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "testset = dataset[10000 :]\n",
    "\n",
    "trainset = dataset[:10000]\n",
    "\n",
    "train_sent_en = trainset[:,1]\n",
    "#train_sent_sv = trainset[:,2]\n",
    "train_lab = trainset[:,0]\n",
    "\n",
    "test_sent_en = testset[:,1]\n",
    "#test_sent_sv = testset[:,2]\n",
    "test_lab = testset[:,0]\n",
    "\n",
    "labels = ['BookRestaurant','GetWeather', 'SearchScreeningEvent','RateBook', 'SearchCreativeWork', 'AddToPlaylist', 'PlayMusic']\n",
    "lab2id = {}\n",
    "id2lab = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    lab2id[labels[i]] = i\n",
    "    id2lab[i] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3784, array(['RateBook',\n",
       "        'I give The Lady Decides a rating value of 4 and a best rating of 6',\n",
       "        'Jag ger Damen Bestämmer sig för ett betyg värde av 4 och en bäst betyg av 6'],\n",
       "       dtype='<U187'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset), trainset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Saving graph from tutorial  https://rentry.co/nxpua\n",
    "\n",
    "from tensorflow.saved_model import simple_save\n",
    "\n",
    "export_dir = \"models/universal_encoder/00000001\"\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\") \n",
    "    print('module downloaded')\n",
    "    input_params = module.get_input_info_dict()\n",
    "\n",
    "    text_input = tf.placeholder(name='text', dtype=input_params['text'].dtype, \n",
    "        shape=input_params['text'].get_shape())\n",
    "    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "\n",
    "    embeddings = module(text_input)\n",
    "\n",
    "    simple_save(sess,\n",
    "        export_dir,\n",
    "        inputs={'text': text_input},\n",
    "        outputs={'embeddings': embeddings},\n",
    "        legacy_init_op=tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_vecs(sents):\n",
    "    \n",
    "    r = requests.post(\n",
    "            'http://localhost:8501/v1/models/universal_encoder:predict', \n",
    "            json={'inputs': {'text': sents}})\n",
    "\n",
    "    responce = r.json() \n",
    "    return responce['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labs(labs):\n",
    "    out = []\n",
    "    for lab in labs:\n",
    "        out.append(lab2id[lab])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = prepare_labs(dataset[:,0])\n",
    "test_labs = prepare_labs(test_lab)\n",
    "\n",
    "train_sent_en = list(pre_process_documents(train_sent_en))\n",
    "\n",
    "train_vecs = []\n",
    "for i in range(0,len(train_sent_en),100):\n",
    "    train_vecs.extend(get_vecs(train_sent_en[i:i+100]))\n",
    "\n",
    "    \n",
    "test_sent_en = list(pre_process_documents(test_sent_en))\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(0,len(test_sent_en),100):\n",
    "    test_vecs.extend(get_vecs(test_sent_en[i:i+100]))\n",
    "    \n",
    "test_vecs = list(map(lambda x: np.array([x]) , test_vecs))\n",
    "train_vecs = list(map(lambda x: np.array([x]) , train_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_en = np.load('data/snips_processed/USE-en.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train_model(train_vecs, train_labs, test_vecs, test_labs, verbose = False, runs = 6001):\n",
    "    net = Baseline(in_size=512)\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    tvecs = torch.tensor(list(train_vecs)).float()\n",
    "    tvecst = torch.tensor(list(test_vecs)).float()\n",
    "    tlabs = torch.tensor(train_labs).long()\n",
    "    tlabst = torch.tensor(test_labs).long()\n",
    "    \n",
    "    t = time.time()\n",
    "    for i in range(runs):\n",
    "        loss = train(net, criterion, optimizer, tlabs, tvecs)\n",
    "    \n",
    "        if verbose and not i% 100:\n",
    "            eval_loss, acc = evaluate(net, tlabst, tvecst, criterion)\n",
    "            print('#{:3d}, {:5d} sec. train loss: {:.7f}, eval loss: {:.4f}, acc = {:.3f}'.format(i, int(time.time() - t), loss, eval_loss, acc))\n",
    "    \n",
    "    eval_loss, acc = evaluate(net, tlabst, tvecst, criterion)\n",
    "    return acc, net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(vecs, labs, folds = 5):\n",
    "    \n",
    "    delims = np.arange(0, len(vecs), len(vecs)//folds)\n",
    "    results = []\n",
    "    t = time.time()\n",
    "    for i in range(folds):\n",
    "        acc, model = full_train_model(np.vstack((vecs[:delims[i]], vecs[delims[i+1]:])),\n",
    "                             np.hstack((labs[:delims[i]], labs[delims[i+1]:])),\n",
    "                             vecs[delims[i] : delims[i+1]],\n",
    "                             labs[delims[i] : delims[i+1]],\n",
    "                                  False, runs = 6001)\n",
    "        \n",
    "        results.append(acc)\n",
    "        print('#{:3d}, {:5d} sec. acc = {:.3f}'.format(i, int(time.time() - t), results[-1]))\n",
    "\n",
    "    return(sum(results)/len(results))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0,    82 sec. acc = 0.972\n",
      "#  1,   171 sec. acc = 0.968\n",
      "#  2,   254 sec. acc = 0.969\n",
      "#  3,   338 sec. acc = 0.967\n",
      "#  4,   421 sec. acc = 0.968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9687953555878084"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv(vecs_en, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0,     0 sec. train loss: 0.0001947, eval loss: 1.9383, acc = 0.197\n",
      "#100,     1 sec. train loss: 0.0001265, eval loss: 1.2599, acc = 0.915\n",
      "#200,     2 sec. train loss: 0.0000860, eval loss: 0.8596, acc = 0.921\n",
      "#300,     3 sec. train loss: 0.0000635, eval loss: 0.6377, acc = 0.928\n",
      "#400,     4 sec. train loss: 0.0000503, eval loss: 0.5076, acc = 0.934\n",
      "#500,     6 sec. train loss: 0.0000419, eval loss: 0.4247, acc = 0.938\n",
      "#600,     7 sec. train loss: 0.0000361, eval loss: 0.3680, acc = 0.943\n",
      "#700,     8 sec. train loss: 0.0000319, eval loss: 0.3270, acc = 0.946\n",
      "#800,     9 sec. train loss: 0.0000287, eval loss: 0.2960, acc = 0.950\n",
      "#900,    11 sec. train loss: 0.0000263, eval loss: 0.2717, acc = 0.952\n",
      "#1000,    12 sec. train loss: 0.0000243, eval loss: 0.2523, acc = 0.953\n",
      "#1100,    13 sec. train loss: 0.0000226, eval loss: 0.2363, acc = 0.954\n",
      "#1200,    14 sec. train loss: 0.0000212, eval loss: 0.2229, acc = 0.957\n",
      "#1300,    15 sec. train loss: 0.0000201, eval loss: 0.2115, acc = 0.958\n",
      "#1400,    17 sec. train loss: 0.0000190, eval loss: 0.2017, acc = 0.958\n",
      "#1500,    18 sec. train loss: 0.0000181, eval loss: 0.1932, acc = 0.959\n",
      "#1600,    19 sec. train loss: 0.0000174, eval loss: 0.1857, acc = 0.959\n",
      "#1700,    20 sec. train loss: 0.0000167, eval loss: 0.1790, acc = 0.960\n",
      "#1800,    21 sec. train loss: 0.0000160, eval loss: 0.1731, acc = 0.960\n",
      "#1900,    23 sec. train loss: 0.0000155, eval loss: 0.1678, acc = 0.961\n",
      "#2000,    24 sec. train loss: 0.0000150, eval loss: 0.1630, acc = 0.961\n",
      "#2100,    25 sec. train loss: 0.0000145, eval loss: 0.1586, acc = 0.962\n",
      "#2200,    26 sec. train loss: 0.0000141, eval loss: 0.1546, acc = 0.962\n",
      "#2300,    27 sec. train loss: 0.0000137, eval loss: 0.1510, acc = 0.963\n",
      "#2400,    29 sec. train loss: 0.0000133, eval loss: 0.1476, acc = 0.964\n",
      "#2500,    30 sec. train loss: 0.0000129, eval loss: 0.1445, acc = 0.964\n",
      "#2600,    31 sec. train loss: 0.0000126, eval loss: 0.1416, acc = 0.965\n",
      "#2700,    32 sec. train loss: 0.0000123, eval loss: 0.1390, acc = 0.964\n",
      "#2800,    34 sec. train loss: 0.0000120, eval loss: 0.1365, acc = 0.964\n",
      "#2900,    35 sec. train loss: 0.0000118, eval loss: 0.1342, acc = 0.964\n",
      "#3000,    36 sec. train loss: 0.0000115, eval loss: 0.1320, acc = 0.965\n",
      "#3100,    37 sec. train loss: 0.0000113, eval loss: 0.1300, acc = 0.965\n",
      "#3200,    39 sec. train loss: 0.0000110, eval loss: 0.1281, acc = 0.965\n",
      "#3300,    40 sec. train loss: 0.0000108, eval loss: 0.1263, acc = 0.966\n",
      "#3400,    42 sec. train loss: 0.0000106, eval loss: 0.1245, acc = 0.966\n",
      "#3500,    43 sec. train loss: 0.0000104, eval loss: 0.1229, acc = 0.966\n",
      "#3600,    45 sec. train loss: 0.0000102, eval loss: 0.1214, acc = 0.966\n",
      "#3700,    46 sec. train loss: 0.0000100, eval loss: 0.1200, acc = 0.966\n",
      "#3800,    47 sec. train loss: 0.0000098, eval loss: 0.1186, acc = 0.966\n",
      "#3900,    48 sec. train loss: 0.0000097, eval loss: 0.1173, acc = 0.966\n",
      "#4000,    49 sec. train loss: 0.0000095, eval loss: 0.1161, acc = 0.966\n",
      "#4100,    51 sec. train loss: 0.0000093, eval loss: 0.1149, acc = 0.967\n",
      "#4200,    52 sec. train loss: 0.0000092, eval loss: 0.1138, acc = 0.967\n",
      "#4300,    53 sec. train loss: 0.0000090, eval loss: 0.1127, acc = 0.967\n",
      "#4400,    54 sec. train loss: 0.0000089, eval loss: 0.1117, acc = 0.967\n",
      "#4500,    56 sec. train loss: 0.0000087, eval loss: 0.1107, acc = 0.967\n",
      "#4600,    57 sec. train loss: 0.0000086, eval loss: 0.1098, acc = 0.967\n",
      "#4700,    58 sec. train loss: 0.0000085, eval loss: 0.1089, acc = 0.967\n",
      "#4800,    59 sec. train loss: 0.0000083, eval loss: 0.1080, acc = 0.967\n",
      "#4900,    60 sec. train loss: 0.0000082, eval loss: 0.1072, acc = 0.967\n",
      "#5000,    62 sec. train loss: 0.0000081, eval loss: 0.1064, acc = 0.967\n",
      "#5100,    63 sec. train loss: 0.0000079, eval loss: 0.1057, acc = 0.967\n",
      "#5200,    64 sec. train loss: 0.0000078, eval loss: 0.1050, acc = 0.967\n",
      "#5300,    65 sec. train loss: 0.0000077, eval loss: 0.1043, acc = 0.967\n",
      "#5400,    66 sec. train loss: 0.0000076, eval loss: 0.1037, acc = 0.967\n",
      "#5500,    68 sec. train loss: 0.0000075, eval loss: 0.1031, acc = 0.968\n",
      "#5600,    69 sec. train loss: 0.0000074, eval loss: 0.1025, acc = 0.968\n",
      "#5700,    70 sec. train loss: 0.0000073, eval loss: 0.1019, acc = 0.968\n",
      "#5800,    71 sec. train loss: 0.0000072, eval loss: 0.1014, acc = 0.967\n",
      "#5900,    72 sec. train loss: 0.0000070, eval loss: 0.1009, acc = 0.968\n",
      "#6000,    74 sec. train loss: 0.0000069, eval loss: 0.1004, acc = 0.968\n"
     ]
    }
   ],
   "source": [
    "acc, net = full_train_model(vecs_en[:10000], labs[:10000], vecs_en[10000:], labs[10000:], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give this album of points', 'book a table at atlantic grill in lofgreen', 'give the current album out of points', 'i give the lady decides a rating value of and a best rating of', 'add la voce to my dubstep dangles dirty playlist', 'play any chanson', 'rate the previous essay four of points', 'give the current novel a one out of rating', 'this current book should get four stars or a rating of']\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "vecs = []\n",
    "\n",
    "a = list(pre_process_documents(dataset[:,1]))\n",
    "print(a[1:10])\n",
    "for i in range(0,len(a),100):\n",
    "    vecs.extend(get_vecs(a[i:i+100]))\n",
    "    print(i//100)\n",
    "    \n",
    "vecs = list(map(lambda x: np.array([x]) , vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptv2s = np.array(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/snips_processed/USE-en',np.array(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = prepare_labs(dataset[:, 0])\n",
    "np.save('data/snips_processed/labs',np.array(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'models/USE-linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.0796285 ,  0.012136  , -0.0135948 , ...,  0.0257535 ,\n",
       "         -0.0545727 , -0.0220814 ]],\n",
       "\n",
       "       [[-0.0503377 , -0.0464248 , -0.00572828, ...,  0.0480531 ,\n",
       "         -0.0832616 , -0.078685  ]],\n",
       "\n",
       "       [[ 0.0322778 ,  0.0720855 , -0.0252778 , ..., -0.00387515,\n",
       "         -0.0430343 ,  0.0628465 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.0255634 ,  0.00681807, -0.0536128 , ...,  0.0411098 ,\n",
       "          0.0321836 , -0.0290044 ]],\n",
       "\n",
       "       [[-0.0690401 ,  0.0629174 , -0.0154625 , ...,  0.0387612 ,\n",
       "         -0.0643083 , -0.0636527 ]],\n",
       "\n",
       "       [[ 0.0332168 , -0.0266076 , -0.00943216, ...,  0.0420742 ,\n",
       "         -0.0401178 , -0.0227483 ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
