{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import contractions\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "with open('data/snips_processed/snips.csv', 'r') as f:\n",
    "    reader = csv.reader(x.replace('\\0', '') for x in f)\n",
    "    for line in reader:\n",
    "        dataset.append(line)\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "testset = dataset[10000 :]\n",
    "\n",
    "trainset = dataset[:10000]\n",
    "\n",
    "train_sent_en = trainset[:,1]\n",
    "train_sent_sv = trainset[:,2]\n",
    "\n",
    "train_lab = trainset[:,0]\n",
    "\n",
    "test_sent_en = testset[:,1]\n",
    "test_sent_sv = testset[:,2]\n",
    "\n",
    "test_lab = testset[:,0]\n",
    "\n",
    "labels = list(set(test_lab))\n",
    "lab2id = {}\n",
    "id2lab = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    lab2id[labels[i]] = i\n",
    "    id2lab[i] = labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'SearchScreeningEvent',\n",
       " 1: 'RateBook',\n",
       " 2: 'AddToPlaylist',\n",
       " 3: 'PlayMusic',\n",
       " 4: 'GetWeather',\n",
       " 5: 'SearchCreativeWork',\n",
       " 6: 'BookRestaurant'}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 11:50:20,047 INFO: char embedding size: 4939\n",
      "2019-04-30 11:50:21,354 INFO: word embedding size: 167642\n",
      "2019-04-30 11:50:31,628 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(167642, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(4939, 50, padding_idx=4936)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2019-04-30 11:50:34,261 INFO: char embedding size: 2638\n",
      "2019-04-30 11:50:37,238 INFO: word embedding size: 240647\n",
      "2019-04-30 11:50:49,403 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(240647, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(2638, 50, padding_idx=2635)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from elmoformanylangs import Embedder\n",
    "\n",
    "en_model = Embedder('models/144')\n",
    "sv_model = Embedder('models/173')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 13:14:28,443 INFO: 1 batches, avg len: 6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [['what', 'is','my','name'],['my','name','is','Jan']]\n",
    "# the list of lists which store the sentences \n",
    "# after segment if necessary.\n",
    "a = en_model.sents2elmo(sents)\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "@np.vectorize\n",
    "def pre_process_text(document):\n",
    "    \n",
    "    # lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    document = remove_accented_chars(document)\n",
    "    \n",
    "    # expand contractions    \n",
    "    document = expand_contractions(document)\n",
    "               \n",
    "    # remove special characters and\\or digits    \n",
    "    # insert spaces between special characters to isolate them    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "    document = remove_special_characters(document, remove_digits=True)  \n",
    "        \n",
    "    # remove extra whitespace\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i want to rate the turbulent term of tyke tiler a',\n",
       "       'when is the great question playing at the closest movie house',\n",
       "       'will mondamin be hot on july', ...,\n",
       "       'give me the movie times for fox theatres',\n",
       "       'can i get the movie times for fox theatres',\n",
       "       'what movies are scheduled in the neighbourhood'], dtype='<U127')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_text(test_sent_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vec(sentence, model):\n",
    "    result = np.zeros((1, 1024))\n",
    "    sentence = pre_process_te(sentence)\n",
    "    sentence = list(map(lambda x: x.split(), sentence\n",
    "    \n",
    "    \n",
    "    for word in sentence:\n",
    "        result += model.get_word_vector(word.lower())\n",
    "    return result/len(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labs(labs):\n",
    "    out = []\n",
    "    for lab in labs:\n",
    "        out.append(lab2id[lab])\n",
    "    return out\n",
    "\n",
    "def prepare_sentence_vecs(sents, lang = 'en'):\n",
    "    \n",
    "    if lang == 'en':\n",
    "        model = en_model\n",
    "        slab = 1\n",
    "    elif lang == 'sv':\n",
    "        model = sv_model\n",
    "        slab = 2\n",
    "    else:\n",
    "        raise RuntimeError('lang is not supported')\n",
    "    vectors = []\n",
    "    \n",
    "    sents = pre_process_text(sents)\n",
    "    sents = list(map(lambda x: x.split(), sents))\n",
    "    vecs = model.sents2elmo(sents)\n",
    "    vecs = list(map(lambda x:[x.mean(axis=0)], vecs))\n",
    "        \n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, in_size = 1024, out_size = 7):\n",
    "        super(Baseline, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(in_size, 7)\n",
    "        self.out = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.W(x)\n",
    "        return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, labels, vectors):\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    #vectors = torch.tensor(vectors).float()\n",
    "    #labels = torch.tensor(labels)\n",
    "    \n",
    "    model_out = model.forward(vectors)\n",
    "    loss += criterion(model_out[:,0], labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, labels, vectors):\n",
    "    with torch.no_grad():\n",
    "        #vectors = torch.tensor(vectors).float()\n",
    "        #labels = torch.tensor(labels)\n",
    "    \n",
    "        model_out = model.forward(vectors)\n",
    "        right = 0\n",
    "        \n",
    "        for i  in range(len(model_out)):\n",
    "            k, v = model_out[i].topk(1)\n",
    "            predicted, true = v.item(), labels[i].item()\n",
    "            if predicted == true:\n",
    "                right +=1\n",
    "\n",
    "                \n",
    "        loss = criterion(model_out[:,0], labels)\n",
    "        return loss.item(), right/len(model_out)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vecs, labss = prepare_sentence_vecs(train_sent_en), prepare_labs(train_lab)\n",
    "#vecst, labst = prepare_sentence_vecs(test_sent_en), prepare_labs(test_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0,     0 sec. train loss: 0.0002009, eval loss: 1.9417, acc = 0.141\n",
      "#100,     2 sec. train loss: 0.0000289, eval loss: 2.0385, acc = 0.607\n",
      "#200,     4 sec. train loss: 0.0000158, eval loss: 2.0120, acc = 0.691\n",
      "#300,     6 sec. train loss: 0.0000112, eval loss: 2.0577, acc = 0.699\n",
      "#400,     9 sec. train loss: 0.0000088, eval loss: 2.1031, acc = 0.702\n",
      "#500,    11 sec. train loss: 0.0000074, eval loss: 2.1404, acc = 0.705\n",
      "#600,    13 sec. train loss: 0.0000064, eval loss: 2.1697, acc = 0.706\n",
      "#700,    15 sec. train loss: 0.0000056, eval loss: 2.1916, acc = 0.708\n",
      "#800,    18 sec. train loss: 0.0000050, eval loss: 2.2073, acc = 0.709\n",
      "#900,    20 sec. train loss: 0.0000046, eval loss: 2.2176, acc = 0.710\n",
      "#1000,    22 sec. train loss: 0.0000042, eval loss: 2.2231, acc = 0.710\n",
      "#1100,    24 sec. train loss: 0.0000038, eval loss: 2.2246, acc = 0.711\n",
      "#1200,    26 sec. train loss: 0.0000035, eval loss: 2.2226, acc = 0.712\n",
      "#1300,    29 sec. train loss: 0.0000033, eval loss: 2.2176, acc = 0.712\n",
      "#1400,    31 sec. train loss: 0.0000031, eval loss: 2.2102, acc = 0.713\n",
      "#1500,    33 sec. train loss: 0.0000029, eval loss: 2.2010, acc = 0.713\n",
      "#1600,    35 sec. train loss: 0.0000027, eval loss: 2.1908, acc = 0.714\n",
      "#1700,    38 sec. train loss: 0.0000025, eval loss: 2.1808, acc = 0.714\n",
      "#1800,    40 sec. train loss: 0.0000024, eval loss: 2.1721, acc = 0.714\n",
      "#1900,    42 sec. train loss: 0.0000022, eval loss: 2.1665, acc = 0.714\n",
      "#2000,    44 sec. train loss: 0.0000021, eval loss: 2.1650, acc = 0.714\n",
      "#2100,    47 sec. train loss: 0.0000020, eval loss: 2.1683, acc = 0.714\n",
      "#2200,    49 sec. train loss: 0.0000019, eval loss: 2.1760, acc = 0.714\n",
      "#2300,    51 sec. train loss: 0.0000018, eval loss: 2.1872, acc = 0.715\n",
      "#2400,    53 sec. train loss: 0.0000017, eval loss: 2.2008, acc = 0.715\n",
      "#2500,    55 sec. train loss: 0.0000016, eval loss: 2.2160, acc = 0.715\n",
      "#2600,    57 sec. train loss: 0.0000015, eval loss: 2.2322, acc = 0.715\n",
      "#2700,    60 sec. train loss: 0.0000014, eval loss: 2.2490, acc = 0.715\n",
      "#2800,    62 sec. train loss: 0.0000013, eval loss: 2.2661, acc = 0.716\n",
      "#2900,    64 sec. train loss: 0.0000013, eval loss: 2.2834, acc = 0.716\n",
      "#3000,    66 sec. train loss: 0.0000012, eval loss: 2.3006, acc = 0.715\n",
      "#3100,    68 sec. train loss: 0.0000012, eval loss: 2.3179, acc = 0.716\n",
      "#3200,    71 sec. train loss: 0.0000011, eval loss: 2.3350, acc = 0.716\n",
      "#3300,    73 sec. train loss: 0.0000010, eval loss: 2.3519, acc = 0.716\n",
      "#3400,    75 sec. train loss: 0.0000010, eval loss: 2.3687, acc = 0.716\n",
      "#3500,    77 sec. train loss: 0.0000009, eval loss: 2.3853, acc = 0.716\n",
      "#3600,    79 sec. train loss: 0.0000009, eval loss: 2.4016, acc = 0.716\n",
      "#3700,    81 sec. train loss: 0.0000008, eval loss: 2.4177, acc = 0.716\n",
      "#3800,    84 sec. train loss: 0.0000008, eval loss: 2.4335, acc = 0.716\n",
      "#3900,    86 sec. train loss: 0.0000008, eval loss: 2.4490, acc = 0.716\n",
      "#4000,    88 sec. train loss: 0.0000007, eval loss: 2.4644, acc = 0.716\n"
     ]
    }
   ],
   "source": [
    "net = Baseline()\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "tvecs = torch.tensor(vecs).float()\n",
    "tvecst = torch.tensor(vecst).float()\n",
    "tlabs = torch.tensor(labs)\n",
    "tlabst = torch.tensor(labst)\n",
    "\n",
    "t = time.time()\n",
    "for i in range(4001):\n",
    "    loss = train(net, criterion, optimizer, tlabs, tvecs)\n",
    "    if not i% 100:\n",
    "        eval_loss, acc = eval(net, tlabst, tvecst)\n",
    "        print('#{:3d}, {:5d} sec. train loss: {:.7f}, eval loss: {:.4f}, acc = {:.3f}'.format(i, int(time.time() - t), loss, eval_loss, acc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0,     0 sec. train loss: 0.0001956, eval loss: 1.9371, acc = 0.193\n",
      "#100,     2 sec. train loss: 0.0001027, eval loss: 1.0330, acc = 0.743\n",
      "#200,     4 sec. train loss: 0.0000766, eval loss: 0.7820, acc = 0.792\n",
      "#300,     6 sec. train loss: 0.0000642, eval loss: 0.6639, acc = 0.818\n",
      "#400,     8 sec. train loss: 0.0000566, eval loss: 0.5932, acc = 0.832\n",
      "#500,    10 sec. train loss: 0.0000514, eval loss: 0.5451, acc = 0.848\n",
      "#600,    13 sec. train loss: 0.0000476, eval loss: 0.5098, acc = 0.856\n",
      "#700,    15 sec. train loss: 0.0000445, eval loss: 0.4824, acc = 0.864\n",
      "#800,    17 sec. train loss: 0.0000420, eval loss: 0.4602, acc = 0.870\n",
      "#900,    19 sec. train loss: 0.0000399, eval loss: 0.4419, acc = 0.873\n",
      "#1000,    22 sec. train loss: 0.0000381, eval loss: 0.4264, acc = 0.877\n",
      "#1100,    24 sec. train loss: 0.0000365, eval loss: 0.4130, acc = 0.880\n",
      "#1200,    26 sec. train loss: 0.0000351, eval loss: 0.4014, acc = 0.883\n",
      "#1300,    29 sec. train loss: 0.0000338, eval loss: 0.3911, acc = 0.886\n",
      "#1400,    31 sec. train loss: 0.0000326, eval loss: 0.3820, acc = 0.886\n",
      "#1500,    33 sec. train loss: 0.0000315, eval loss: 0.3738, acc = 0.888\n",
      "#1600,    35 sec. train loss: 0.0000305, eval loss: 0.3665, acc = 0.890\n",
      "#1700,    38 sec. train loss: 0.0000296, eval loss: 0.3598, acc = 0.890\n",
      "#1800,    40 sec. train loss: 0.0000288, eval loss: 0.3538, acc = 0.892\n",
      "#1900,    42 sec. train loss: 0.0000279, eval loss: 0.3482, acc = 0.893\n",
      "#2000,    44 sec. train loss: 0.0000272, eval loss: 0.3432, acc = 0.895\n",
      "#2100,    47 sec. train loss: 0.0000265, eval loss: 0.3385, acc = 0.895\n",
      "#2200,    49 sec. train loss: 0.0000258, eval loss: 0.3342, acc = 0.896\n",
      "#2300,    51 sec. train loss: 0.0000252, eval loss: 0.3303, acc = 0.897\n",
      "#2400,    53 sec. train loss: 0.0000245, eval loss: 0.3266, acc = 0.897\n",
      "#2500,    56 sec. train loss: 0.0000240, eval loss: 0.3232, acc = 0.898\n",
      "#2600,    58 sec. train loss: 0.0000234, eval loss: 0.3201, acc = 0.899\n",
      "#2700,    61 sec. train loss: 0.0000229, eval loss: 0.3171, acc = 0.899\n",
      "#2800,    63 sec. train loss: 0.0000223, eval loss: 0.3144, acc = 0.900\n",
      "#2900,    65 sec. train loss: 0.0000218, eval loss: 0.3119, acc = 0.901\n",
      "#3000,    67 sec. train loss: 0.0000214, eval loss: 0.3095, acc = 0.901\n",
      "#3100,    70 sec. train loss: 0.0000209, eval loss: 0.3074, acc = 0.903\n",
      "#3200,    73 sec. train loss: 0.0000204, eval loss: 0.3053, acc = 0.902\n",
      "#3300,    75 sec. train loss: 0.0000200, eval loss: 0.3034, acc = 0.904\n",
      "#3400,    77 sec. train loss: 0.0000196, eval loss: 0.3017, acc = 0.905\n",
      "#3500,    80 sec. train loss: 0.0000192, eval loss: 0.3001, acc = 0.905\n",
      "#3600,    82 sec. train loss: 0.0000188, eval loss: 0.2986, acc = 0.906\n",
      "#3700,    84 sec. train loss: 0.0000184, eval loss: 0.2972, acc = 0.906\n",
      "#3800,    87 sec. train loss: 0.0000180, eval loss: 0.2959, acc = 0.905\n",
      "#3900,    89 sec. train loss: 0.0000176, eval loss: 0.2947, acc = 0.905\n",
      "#4000,    91 sec. train loss: 0.0000173, eval loss: 0.2936, acc = 0.905\n"
     ]
    }
   ],
   "source": [
    "net_sv = Baseline()\n",
    "optimizer = torch.optim.Adam(net_sv.parameters())\n",
    "criterion = torch.nn.NLLLoss()\n",
    "#labs, vecs = prepare_pairs(trainset, lang = 'en')\n",
    "#labst, vecst = prepare_pairs(testset, lang = 'en')\n",
    "sv_vecs_train = torch.tensor(sv_vecs[:10000]).float()\n",
    "sv_vecs_test = torch.tensor(sv_vecs[10000:]).float()\n",
    "train_labs = torch.tensor(prepare_labs(train_lab))\n",
    "test_labs = torch.tensor(prepare_labs(test_lab))\n",
    "\n",
    "t = time.time()\n",
    "for i in range(4001):\n",
    "    loss = train(net_sv, criterion, optimizer, train_labs, sv_vecs_train)\n",
    "    if not i% 100:\n",
    "        eval_loss, acc = eval(net_sv, test_labs, sv_vecs_test)\n",
    "        print('#{:3d}, {:5d} sec. train loss: {:.7f}, eval loss: {:.4f}, acc = {:.3f}'.format(i, int(time.time() - t), loss, eval_loss, acc))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_visual(model, labels, vectors):\n",
    "    with torch.no_grad():\n",
    "        vectors = torch.tensor(vectors).float()\n",
    "        labels = torch.tensor(labels)\n",
    "    \n",
    "        model_out = model.forward(vectors)\n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        for i  in range(len(model_out)):\n",
    "            k, v = model_out[i].topk(1)\n",
    "            predicted, true = v.item(), labels[i].item()\n",
    "            if predicted == true:\n",
    "                right +=1\n",
    "            else:\n",
    "                print(id2lab[predicted], id2lab[true])\n",
    "                wrong +=1\n",
    "                \n",
    "        print('{} out of {} = {}'.format(right, right+wrong, right/(right+wrong)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComparePlaces GetPlaceDetails\n",
      "SearchPlace RequestRide\n",
      "BookRestaurant GetWeather\n",
      "GetPlaceDetails GetWeather\n",
      "RequestRide GetTrafficInformation\n",
      "SearchPlace GetPlaceDetails\n",
      "GetTrafficInformation GetDirections\n",
      "GetPlaceDetails GetWeather\n",
      "GetDirections GetPlaceDetails\n",
      "GetWeather GetPlaceDetails\n",
      "GetDirections SearchPlace\n",
      "SearchPlace ShareETA\n",
      "SearchPlace GetPlaceDetails\n",
      "GetPlaceDetails GetTrafficInformation\n",
      "GetTrafficInformation GetWeather\n",
      "GetWeather ShareCurrentLocation\n",
      "GetPlaceDetails ComparePlaces\n",
      "SearchPlace BookRestaurant\n",
      "GetPlaceDetails GetTrafficInformation\n",
      "SearchPlace RequestRide\n",
      "ComparePlaces GetPlaceDetails\n",
      "GetWeather GetPlaceDetails\n",
      "GetDirections GetPlaceDetails\n",
      "GetWeather GetPlaceDetails\n",
      "GetPlaceDetails GetWeather\n",
      "RequestRide GetWeather\n",
      "BookRestaurant GetWeather\n",
      "BookRestaurant GetTrafficInformation\n",
      "50 out of 78 = 0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "eval_visual(net, labst, vecst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['RateBook', 'rate the current textbook zero out of 6 points'],\n",
       "       ['BookRestaurant',\n",
       "        'restaurant in Elberta for alma, deana and olga at 18:49:20 that serves tsipouro'],\n",
       "       ['SearchScreeningEvent',\n",
       "        'What movies are currently at Star Theatres?'],\n",
       "       ...,\n",
       "       ['PlayMusic', 'Please play Different Slanguages by Fred Labour.'],\n",
       "       ['PlayMusic', 'play some King Tubby from the eighties'],\n",
       "       ['AddToPlaylist',\n",
       "        'add M-CABI to the playlist named Pre-Party R&B Jams']],\n",
       "      dtype='<U186')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(np.squeeze(np.array(vecs), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "svp = pre_process_text(dataset[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-30 11:54:03,067 INFO: 216 batches, avg len: 10.4\n",
      "2019-04-30 11:55:08,015 INFO: Finished 1000 sentences.\n",
      "2019-04-30 11:56:04,033 INFO: Finished 2000 sentences.\n",
      "2019-04-30 11:56:57,281 INFO: Finished 3000 sentences.\n",
      "2019-04-30 11:58:15,155 INFO: Finished 4000 sentences.\n",
      "2019-04-30 11:59:09,727 INFO: Finished 5000 sentences.\n",
      "2019-04-30 11:59:59,573 INFO: Finished 6000 sentences.\n",
      "2019-04-30 12:01:17,737 INFO: Finished 7000 sentences.\n",
      "2019-04-30 12:02:22,889 INFO: Finished 8000 sentences.\n",
      "2019-04-30 12:03:17,264 INFO: Finished 9000 sentences.\n",
      "2019-04-30 12:04:13,081 INFO: Finished 10000 sentences.\n",
      "2019-04-30 12:05:06,521 INFO: Finished 11000 sentences.\n",
      "2019-04-30 12:06:10,445 INFO: Finished 12000 sentences.\n",
      "2019-04-30 12:07:10,576 INFO: Finished 13000 sentences.\n"
     ]
    }
   ],
   "source": [
    "sv_vecs = prepare_sentence_vecs(dataset[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labs), len(sv_vecs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('data/snips_processed/ELMO-sv',np.array(sv_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(np.squeeze(np.array(sv_vecs), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
